{% set name = "cw-eval" %}
{% set version = "1.0.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name|replace("-","_") }}-{{ version }}.tar.gz
  sha256: ca9e235048b1104bd64e70b935a7eec35150a27d1544fe2e9f71e476ceed73c3


build:
  number: 1000
  script: "{{ PYTHON }} -m pip install . --no-deps -vv"

requirements:
  build:
    - {{ compiler('c') }}
  host:
    - python
    - pip
  run:
    - python
    - rtree
    - shapely
    - pandas
    - geopandas
    - tqdm

test:
  imports:
    cw_eval

about:
  home: http://github.com/cosmiq/cw-eval
  license: Apache-2.0
  license_family: Apache
  license_file: '{{ environ["RECIPE_DIR"] }}/LICENSE.txt'
  summary: 'Evaluation metrics for Geospatial Machine Learning Challenges'

  description: |
    cw-eval is an evaluation suite for scoring entries in geospatial image
    analysis competitions. It includes tools for calculating IoU scores,
    precision, recall, F1 score, and scripts to score entire entries in either
    geojson or csv formats.
  doc_url: http://cw-eval.readthedocs.io/
  dev_url: https://github.com/cosmiq/cw-eval

extra:
  recipe-maintainers:
    - nrweir
